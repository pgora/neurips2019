{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of fit and evaluate experiments for simple feedforward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukasz\\AppData\\Local\\conda\\conda\\envs\\aind\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import graph_utils as graph_utils\n",
    "import graph_neural_networks as graph_nn\n",
    "import data_preparation_utils as data_prep\n",
    "from iterative_updaters import VanillaGradientDescent, MomentumGradientDescent, NesterovMomentumGradientDescent, RMSPropGradientDescent, AdamGradientDescent\n",
    "import training_and_evaluation as train_eval\n",
    "import graph_nn_experiments as experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function constructs a simple feedforward neural net with the specified number of hidden layers and neurons\n",
    "def simple_feedforward_neural_net(inpt, no_of_layers, no_of_neurons, activation_function):\n",
    "    hidden = tf.layers.dense(inpt, no_of_neurons, activation=activation_function, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    for i in range(no_of_layers - 1):\n",
    "        hidden = tf.layers.dense(hidden, no_of_neurons, activation=activation_function, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return tf.layers.dense(hidden, 1, kernel_initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100k_feedforward_fit_and_evaluate_experiments/simulation_Xy_10_11_relu.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_directory = \"100k_feedforward_fit_and_evaluate_experiments\"\n",
    "\n",
    "def gradient_descent_set_file_name(no_of_layers, no_of_channels, activation):\n",
    "    return \"%s/simulation_Xy_%d_%d_%s.csv\" % (experiments_directory, no_of_layers, no_of_channels, activation)\n",
    "\n",
    "gradient_descent_set_file_name(10, 11, \"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work fine. Unfortunately, I haven't saved these file names to fit_eval_results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.022308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.026668</td>\n",
       "      <td>0.023650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.029692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027142</td>\n",
       "      <td>0.031580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.033653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>0.037320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027180</td>\n",
       "      <td>0.048057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.074120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.029323</td>\n",
       "      <td>0.078345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1     2                                                  3         4  \\\n",
       "5  2   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019872   \n",
       "6  3   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.026668   \n",
       "1  3   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019230   \n",
       "2  4   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027142   \n",
       "4  2  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018455   \n",
       "7  4   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018923   \n",
       "0  2   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027180   \n",
       "8  3  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.024013   \n",
       "3  4  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.029323   \n",
       "\n",
       "          5  \n",
       "5  0.022308  \n",
       "6  0.023650  \n",
       "1  0.029692  \n",
       "2  0.031580  \n",
       "4  0.033653  \n",
       "7  0.037320  \n",
       "0  0.048057  \n",
       "8  0.074120  \n",
       "3  0.078345  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"100k_feedforward_fit_and_evaluate_experiments/fit_eval_results.csv\", header=None)\n",
    "df.sort_values([5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the original training data for scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>14</td>\n",
       "      <td>58</td>\n",
       "      <td>108</td>\n",
       "      <td>83</td>\n",
       "      <td>60</td>\n",
       "      <td>82</td>\n",
       "      <td>61</td>\n",
       "      <td>64</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>87</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>77</td>\n",
       "      <td>95</td>\n",
       "      <td>5</td>\n",
       "      <td>49082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>92</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>95</td>\n",
       "      <td>64</td>\n",
       "      <td>110</td>\n",
       "      <td>98</td>\n",
       "      <td>95</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>105</td>\n",
       "      <td>118</td>\n",
       "      <td>48</td>\n",
       "      <td>40</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>51223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>66</td>\n",
       "      <td>104</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>115</td>\n",
       "      <td>118</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>51461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>65</td>\n",
       "      <td>108</td>\n",
       "      <td>67</td>\n",
       "      <td>81</td>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>83</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>79</td>\n",
       "      <td>44</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>46</td>\n",
       "      <td>102</td>\n",
       "      <td>62</td>\n",
       "      <td>45119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>87</td>\n",
       "      <td>78</td>\n",
       "      <td>99</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>102</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>50660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2    3   4   5    6   7   8    9   ...     12   13   14   15   16  \\\n",
       "0   81  14  58  108  83  60   82  61  64   94  ...     28   87   73   69   25   \n",
       "1   81  92  16   44  95  64  110  98  95  105  ...     42    5   40  105  118   \n",
       "2   75  20  66  104  20  65   80  97   2   81  ...    119  115  118   64   68   \n",
       "3   30  42  65  108  67  81   85  32  83   48  ...    107   79   44   90    6   \n",
       "4  117  20  52   40  25  36   87  78  99   70  ...     92   21   34  102   37   \n",
       "\n",
       "   17  18   19  20     21  \n",
       "0   7  77   95   5  49082  \n",
       "1  48  40   62  45  51223  \n",
       "2  46  45   43  18  51461  \n",
       "3  25  46  102  62  45119  \n",
       "4  46  11    9  66  50660  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_lights_data = pd.read_csv(\"100k.csv\", header=None)\n",
    "traffic_lights_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukasz\\AppData\\Local\\conda\\conda\\envs\\aind\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_scaler, y_scaler = data_prep.scale_standard_traffic_light_data(traffic_lights_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48922.72721576669"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(traffic_lights_data[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cf98822748>]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps = [i / 4.0 for i in range(401)]\n",
    "percentiles=-np.percentile(-traffic_lights_data[21], ps)\n",
    "plt.xlim(65000.0,35000.0)\n",
    "plt.xlabel(\"Simulator output\")\n",
    "plt.ylabel(\"Percentile in 100k dataset\")\n",
    "plt.plot(percentiles, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=831191)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code not really used, just kept in case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gradient_descent_data(no_of_layers, no_of_channels, activation_name):\n",
    "    data_file_name = gradient_descent_set_file_name(no_of_layers, no_of_channels, activation_name)\n",
    "    data = pd.read_csv(data_file_name, header=None)\n",
    "    no_of_columns = data.shape[1]\n",
    "    X = data.iloc[:,0:(no_of_columns-1)]\n",
    "    y = data.iloc[:,no_of_columns-1].values.reshape(-1,1)\n",
    "    \n",
    "    data_normalized = data.copy()\n",
    "    data_normalized.iloc[:,0:(no_of_columns-1)] = X_scaler.transform(data_normalized.iloc[:,0:(no_of_columns-1)])\n",
    "    data_normalized.iloc[:,no_of_columns-1] = y_scaler.transform(data_normalized.iloc[:,no_of_columns-1].values.reshape(-1,1))\n",
    "    X_scaled = data_normalized.iloc[:,0:(no_of_columns-1)]\n",
    "    y_scaled = data_normalized.iloc[:,no_of_columns-1]\n",
    "    \n",
    "    return X, y, X_scaled, y_scaled\n",
    "\n",
    "def scale_data(X_input, y_input, X_scaler, y_scaler):\n",
    "    X_input_ = X_input.copy()\n",
    "    y_input_ = y_input.copy()\n",
    "    X_scaled = X_scaler.transform(X_input_)\n",
    "    y_scaled = y_scaler.transform(y_input_)\n",
    "    return X_scaled, y_scaled\n",
    "    \n",
    "    \n",
    "def find_lowest_avg_waiting_time(no_of_layers, no_of_channels, activation_name):\n",
    "    X, y, X_scaled, y_scaled = load_gradient_descent_data(no_of_layers, no_of_channels, activation_name)\n",
    "    return min(y)[0]\n",
    "\n",
    "def assess_avg_error_below_some_target_values(target_values, \n",
    "                                              no_of_layers,\n",
    "                                              no_of_neurons, \n",
    "                                              activation_name,\n",
    "                                              model_file_name,\n",
    "                                              X_scaler, \n",
    "                                              y_scaler):\n",
    "    \n",
    "    g_X, g_y, scaled_g_X, scaled_g_y = load_gradient_descent_data(no_of_layers, no_of_neurons, activation_name)\n",
    "    #scaled_g_X, scaled_g_y = scale_data(g_X, g_y, X_scaler, y_scaler)\n",
    "    \n",
    "    if activation_name == \"relu\":\n",
    "        activation = tf.nn.relu\n",
    "    else:\n",
    "        activation = tf.nn.tanh\n",
    "        \n",
    "    tf.reset_default_graph()\n",
    "    nn_input = tf.placeholder(dtype=tf.float32, shape=[None, 21])\n",
    "    targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "    print(\"Constructing network with %d layers, %d neurons per layer and %s activation function\" % (no_of_layers, no_of_neurons, activation_name))\n",
    "    nn_output = simple_feedforward_neural_net(nn_input, no_of_layers, no_of_neurons, activation)\n",
    "    \n",
    "    avg_errors = []\n",
    "    \n",
    "    for target_value in target_values:\n",
    "        \n",
    "        selected_indices = np.argwhere(g_y.reshape(-1) < target_value).reshape(-1)\n",
    "        \n",
    "        if len(selected_indices) > 0:\n",
    "            X_test = scaled_g_X.iloc[selected_indices,:]\n",
    "            y_test = scaled_g_y[selected_indices]\n",
    "            model_avg_error, actual_vs_predicted = train_eval.evaluate_model_on_a_dataset(model_file_name, nn_output, nn_input, X_test, y_test, y_scaler)\n",
    "        else:\n",
    "            model_avg_error = np.nan\n",
    "            \n",
    "        avg_errors.append(model_avg_error)\n",
    "        \n",
    "    return avg_errors\n",
    "\n",
    "def find_maximum_relative_error(no_of_layers, no_of_neurons, activation_name, model_file_name, X_scaler, y_scaler):\n",
    "    \n",
    "    if activation_name == \"relu\":\n",
    "        activation = tf.nn.relu\n",
    "    else:\n",
    "        activation = tf.nn.tanh\n",
    "        \n",
    "    tf.reset_default_graph()\n",
    "    nn_input = tf.placeholder(dtype=tf.float32, shape=[None, 21])\n",
    "    targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "    print(\"Constructing network with %d layers, %d channels per layer and %s activation function\" % (no_of_layers, no_of_channels, activation_name))\n",
    "    nn_output = simple_feedforward_neural_net(nn_input, no_of_layers, no_of_neurons, activation)\n",
    "    \n",
    "    model_max_error = train_eval.find_model_maximum_relative_error_on_a_dataset(model_file_name, nn_output, nn_input, X_test, y_test, y_scaler)\n",
    "    \n",
    "    return model_max_error\n",
    "\n",
    "def assess_error_stdev_below_some_target_values(target_values, \n",
    "                                                no_of_layers,\n",
    "                                                no_of_neurons, \n",
    "                                                activation_name,\n",
    "                                                model_file_name,\n",
    "                                                X_scaler, \n",
    "                                                y_scaler):\n",
    "    \n",
    "    g_X, g_y, scaled_g_X, scaled_g_y = load_gradient_descent_data(no_of_layers, no_of_neurons, activation_name)\n",
    "    #scaled_g_X, scaled_g_y = scale_data(g_X, g_y, X_scaler, y_scaler)\n",
    "    \n",
    "    if activation_name == \"relu\":\n",
    "        activation = tf.nn.relu\n",
    "    else:\n",
    "        activation = tf.nn.tanh\n",
    "        \n",
    "    tf.reset_default_graph()\n",
    "    nn_input = tf.placeholder(dtype=tf.float32, shape=[None, 21])\n",
    "    targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "    print(\"Constructing network with %d layers, %d channels per layer and %s activation function\" % (no_of_layers, no_of_channels, activation_name))\n",
    "    nn_output = simple_feedforward_neural_net(nn_input, no_of_layers, no_of_neurons, activation)\n",
    "    \n",
    "    error_stdevs = []\n",
    "    \n",
    "    for target_value in target_values:\n",
    "        \n",
    "        selected_indices = np.argwhere(g_y.reshape(-1) < target_value).reshape(-1)\n",
    "        \n",
    "        if len(selected_indices) > 0:\n",
    "            X_test = scaled_g_X.iloc[selected_indices,:]\n",
    "            y_test = scaled_g_y[selected_indices]\n",
    "            model_error_stdev = train_eval.find_model_relative_error_stdev_on_a_dataset(model_file_name, nn_output, nn_input, X_test, y_test, y_scaler)\n",
    "        else:\n",
    "            model_error_stdev = np.nan\n",
    "            \n",
    "        error_stdevs.append(model_error_stdev)\n",
    "        \n",
    "    return error_stdevs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add min waiting times (from actual simulation) to the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_waiting_times = []\n",
    "\n",
    "for row in df.iterrows():\n",
    "    no_of_layers = row[1][0]\n",
    "    no_of_channels = row[1][1]\n",
    "    activation_name = row[1][2]\n",
    "    min_waiting_time = find_lowest_avg_waiting_time(no_of_layers, no_of_channels, activation_name)\n",
    "    min_waiting_times.append(min_waiting_time)\n",
    "\n",
    "df_plus_min_waiting_time = df.copy() \n",
    "df_plus_min_waiting_time[len(df_plus_min_waiting_time.columns)] = pd.Series(min_waiting_times, df_plus_min_waiting_time.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>32978.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.026668</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>33922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>33536.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027142</td>\n",
       "      <td>0.031580</td>\n",
       "      <td>33941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.033653</td>\n",
       "      <td>32921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>33117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027180</td>\n",
       "      <td>0.048057</td>\n",
       "      <td>35098.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>34516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.029323</td>\n",
       "      <td>0.078345</td>\n",
       "      <td>35817.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1     2                                                  3         4  \\\n",
       "5  2   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019872   \n",
       "6  3   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.026668   \n",
       "1  3   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019230   \n",
       "2  4   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027142   \n",
       "4  2  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018455   \n",
       "7  4   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018923   \n",
       "0  2   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027180   \n",
       "8  3  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.024013   \n",
       "3  4  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.029323   \n",
       "\n",
       "          5        6  \n",
       "5  0.022308  32978.0  \n",
       "6  0.023650  33922.0  \n",
       "1  0.029692  33536.0  \n",
       "2  0.031580  33941.0  \n",
       "4  0.033653  32921.0  \n",
       "7  0.037320  33117.0  \n",
       "0  0.048057  35098.0  \n",
       "8  0.074120  34516.0  \n",
       "3  0.078345  35817.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plus_min_waiting_time.sort_values([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = next(df.iterrows())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing network with 2 layers, 20 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_20_tanh.ckpt\n",
      "Constructing network with 3 layers, 40 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "Constructing network with 4 layers, 20 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "Constructing network with 4 layers, 100 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_100_tanh.ckpt\n",
      "Constructing network with 2 layers, 100 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "Constructing network with 2 layers, 40 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "Constructing network with 3 layers, 20 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "Constructing network with 4 layers, 40 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "Constructing network with 3 layers, 100 neurons per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_100_tanh.ckpt\n"
     ]
    }
   ],
   "source": [
    "avg_errors_list = []\n",
    "for row in df.iterrows():\n",
    "    r = row[1]\n",
    "    avg_errors = assess_avg_error_below_some_target_values([37000.0,36000.0,35000.0,34000.0,33000.0,32000.0], \n",
    "                                              r[0],\n",
    "                                              r[1], \n",
    "                                              r[2],\n",
    "                                              r[3],\n",
    "                                              X_scaler, \n",
    "                                              y_scaler)\n",
    "    avg_errors_list.append(avg_errors)\n",
    "    \n",
    "avg_error_list = np.array(avg_errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_errors_df =pd.DataFrame(avg_error_list,columns= [\"37000.0\",\"36000.0\",\"35000.0\",\"34000.0\",\"33000.0\",\"32000.0\"]\n",
    "                           )\n",
    "df_with_avg_errors = df_plus_min_waiting_time.copy()\n",
    "df_with_avg_errors = pd.concat([df_with_avg_errors, avg_errors_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>37000.0</th>\n",
       "      <th>36000.0</th>\n",
       "      <th>35000.0</th>\n",
       "      <th>34000.0</th>\n",
       "      <th>33000.0</th>\n",
       "      <th>32000.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>32978.0</td>\n",
       "      <td>0.018482</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.025894</td>\n",
       "      <td>0.038842</td>\n",
       "      <td>0.067634</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.026668</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>33922.0</td>\n",
       "      <td>0.021631</td>\n",
       "      <td>0.040870</td>\n",
       "      <td>0.067909</td>\n",
       "      <td>0.103391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>0.024746</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.032886</td>\n",
       "      <td>0.052574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027142</td>\n",
       "      <td>0.031580</td>\n",
       "      <td>33941.0</td>\n",
       "      <td>0.082059</td>\n",
       "      <td>0.112161</td>\n",
       "      <td>0.129140</td>\n",
       "      <td>0.144566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.033653</td>\n",
       "      <td>32921.0</td>\n",
       "      <td>0.026313</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.043340</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>33117.0</td>\n",
       "      <td>0.045803</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>0.069732</td>\n",
       "      <td>0.085912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027180</td>\n",
       "      <td>0.048057</td>\n",
       "      <td>35098.0</td>\n",
       "      <td>0.075015</td>\n",
       "      <td>0.099408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>34516.0</td>\n",
       "      <td>0.040982</td>\n",
       "      <td>0.015150</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.029323</td>\n",
       "      <td>0.078345</td>\n",
       "      <td>35817.0</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1     2                                                  3         4  \\\n",
       "5  2   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019872   \n",
       "6  3   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.026668   \n",
       "1  3   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019230   \n",
       "2  4   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027142   \n",
       "4  2  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018455   \n",
       "7  4   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018923   \n",
       "0  2   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027180   \n",
       "8  3  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.024013   \n",
       "3  4  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.029323   \n",
       "\n",
       "          5        6   37000.0   36000.0   35000.0   34000.0   33000.0  \\\n",
       "5  0.022308  32978.0  0.018482  0.017908  0.025894  0.038842  0.067634   \n",
       "6  0.023650  33922.0  0.021631  0.040870  0.067909  0.103391       NaN   \n",
       "1  0.029692  33536.0  0.024746  0.021912  0.032886  0.052574       NaN   \n",
       "2  0.031580  33941.0  0.082059  0.112161  0.129140  0.144566       NaN   \n",
       "4  0.033653  32921.0  0.026313  0.018949  0.018100  0.025191  0.043340   \n",
       "7  0.037320  33117.0  0.045803  0.056566  0.069732  0.085912       NaN   \n",
       "0  0.048057  35098.0  0.075015  0.099408       NaN       NaN       NaN   \n",
       "8  0.074120  34516.0  0.040982  0.015150  0.007757       NaN       NaN   \n",
       "3  0.078345  35817.0  0.013072  0.011971       NaN       NaN       NaN   \n",
       "\n",
       "   32000.0  \n",
       "5      NaN  \n",
       "6      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "4      NaN  \n",
       "7      NaN  \n",
       "0      NaN  \n",
       "8      NaN  \n",
       "3      NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_avg_errors.sort_values([5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also calculate max error on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing network with 2 layers, 20 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_20_tanh.ckpt\n",
      "Constructing network with 3 layers, 40 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "Constructing network with 4 layers, 20 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "Constructing network with 4 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_100_tanh.ckpt\n",
      "Constructing network with 2 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "Constructing network with 2 layers, 40 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "Constructing network with 3 layers, 20 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "Constructing network with 4 layers, 40 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "Constructing network with 3 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_100_tanh.ckpt\n"
     ]
    }
   ],
   "source": [
    "max_errors = []\n",
    "\n",
    "for row in df.iterrows():\n",
    "    no_of_layers = row[1][0]\n",
    "    no_of_channels = row[1][1]\n",
    "    activation_name = row[1][2]\n",
    "    model_file = row[1][3]\n",
    "    max_error = find_maximum_relative_error(no_of_layers, no_of_channels, activation_name, model_file, X_scaler, y_scaler)\n",
    "    max_errors.append(max_error)\n",
    "\n",
    "df_plus_max_errors = df_plus_min_waiting_time.copy() \n",
    "df_plus_max_errors[len(df_plus_max_errors.columns)] = pd.Series(max_errors, df_plus_max_errors.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add stratified error stdev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing network with 2 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_20_tanh.ckpt\n",
      "Constructing network with 3 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_40_tanh.ckpt\n",
      "Constructing network with 4 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_20_tanh.ckpt\n",
      "Constructing network with 4 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_100_tanh.ckpt\n",
      "Constructing network with 2 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_100_tanh.ckpt\n",
      "Constructing network with 2 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_2_40_tanh.ckpt\n",
      "Constructing network with 3 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_20_tanh.ckpt\n",
      "Constructing network with 4 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_4_40_tanh.ckpt\n",
      "Constructing network with 3 layers, 100 channels per layer and tanh activation function\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_100_tanh.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 100k_feedforward_fit_and_evaluate_experiments/model_3_100_tanh.ckpt\n"
     ]
    }
   ],
   "source": [
    "error_stdev_list = []\n",
    "for row in df.iterrows():\n",
    "    r = row[1]\n",
    "    error_stdev = assess_error_stdev_below_some_target_values([37000.0,36000.0,35000.0,34000.0,33000.0,32000.0], \n",
    "                                              r[0],\n",
    "                                              r[1], \n",
    "                                              r[2],\n",
    "                                              r[3],\n",
    "                                              X_scaler, \n",
    "                                              y_scaler)\n",
    "    error_stdev_list.append(error_stdev)\n",
    "    \n",
    "error_stdev_list = np.array(error_stdev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_stdev_df = pd.DataFrame(error_stdev_list,columns= [\"37000.0\",\"36000.0\",\"35000.0\",\"34000.0\",\"33000.0\",\"32000.0\"]\n",
    "                           )\n",
    "df_with_error_stdev = df_plus_min_waiting_time.copy()\n",
    "df_with_error_stdev = pd.concat([df_with_error_stdev, error_stdev_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>37000.0</th>\n",
       "      <th>36000.0</th>\n",
       "      <th>35000.0</th>\n",
       "      <th>34000.0</th>\n",
       "      <th>33000.0</th>\n",
       "      <th>32000.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>32978.0</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.015788</td>\n",
       "      <td>0.015798</td>\n",
       "      <td>0.008256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.026668</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>33922.0</td>\n",
       "      <td>0.018218</td>\n",
       "      <td>0.014020</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>33536.0</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>0.017056</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.015021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027142</td>\n",
       "      <td>0.031580</td>\n",
       "      <td>33941.0</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.033653</td>\n",
       "      <td>32921.0</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.013697</td>\n",
       "      <td>0.011323</td>\n",
       "      <td>0.010599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>33117.0</td>\n",
       "      <td>0.022896</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.013304</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.027180</td>\n",
       "      <td>0.048057</td>\n",
       "      <td>35098.0</td>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>34516.0</td>\n",
       "      <td>0.024625</td>\n",
       "      <td>0.012142</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>100k_feedforward_fit_and_evaluate_experiments/...</td>\n",
       "      <td>0.029323</td>\n",
       "      <td>0.078345</td>\n",
       "      <td>35817.0</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1     2                                                  3         4  \\\n",
       "5  2   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019872   \n",
       "6  3   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.026668   \n",
       "1  3   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.019230   \n",
       "2  4   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027142   \n",
       "4  2  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018455   \n",
       "7  4   40  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.018923   \n",
       "0  2   20  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.027180   \n",
       "8  3  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.024013   \n",
       "3  4  100  tanh  100k_feedforward_fit_and_evaluate_experiments/...  0.029323   \n",
       "\n",
       "          5        6   37000.0   36000.0   35000.0   34000.0  33000.0  32000.0  \n",
       "5  0.022308  32978.0  0.015184  0.015788  0.015798  0.008256      0.0      NaN  \n",
       "6  0.023650  33922.0  0.018218  0.014020  0.011570  0.000000      NaN      NaN  \n",
       "1  0.029692  33536.0  0.016581  0.017056  0.018663  0.015021      NaN      NaN  \n",
       "2  0.031580  33941.0  0.024354  0.019723  0.009869  0.000000      NaN      NaN  \n",
       "4  0.033653  32921.0  0.017822  0.013697  0.011323  0.010599      0.0      NaN  \n",
       "7  0.037320  33117.0  0.022896  0.018890  0.013304  0.010028      NaN      NaN  \n",
       "0  0.048057  35098.0  0.021346  0.007974       NaN       NaN      NaN      NaN  \n",
       "8  0.074120  34516.0  0.024625  0.012142  0.003082       NaN      NaN      NaN  \n",
       "3  0.078345  35817.0  0.011159  0.009500       NaN       NaN      NaN      NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_error_stdev.sort_values([5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTeX tables and CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{rrlrrrrrrrrr}\\n\\\\toprule\\nLayers & Channels & Activation & Min sim &  Test &   Sim & Sim<37000.0 & Sim<36000.0 & Sim<35000.0 & Sim<34000.0 & Sim<33000.0 & Sim<32000.0 \\\\\\\\\\n\\\\midrule\\n     2 &      100 &       tanh &   32921 & 1.85\\\\% & 3.37\\\\% &       2.63\\\\% &       1.89\\\\% &       1.81\\\\% &       2.52\\\\% &       4.33\\\\% &        nan\\\\% \\\\\\\\\\n     2 &       40 &       tanh &   32978 & 1.99\\\\% & 2.23\\\\% &       1.85\\\\% &       1.79\\\\% &       2.59\\\\% &       3.88\\\\% &       6.76\\\\% &        nan\\\\% \\\\\\\\\\n     4 &       40 &       tanh &   33117 & 1.89\\\\% & 3.73\\\\% &       4.58\\\\% &       5.66\\\\% &       6.97\\\\% &       8.59\\\\% &        nan\\\\% &        nan\\\\% \\\\\\\\\\n     3 &       40 &       tanh &   33536 & 1.92\\\\% & 2.97\\\\% &       2.47\\\\% &       2.19\\\\% &       3.29\\\\% &       5.26\\\\% &        nan\\\\% &        nan\\\\% \\\\\\\\\\n     3 &       20 &       tanh &   33922 & 2.67\\\\% & 2.37\\\\% &       2.16\\\\% &       4.09\\\\% &       6.79\\\\% &      10.34\\\\% &        nan\\\\% &        nan\\\\% \\\\\\\\\\n     4 &       20 &       tanh &   33941 & 2.71\\\\% & 3.16\\\\% &       8.21\\\\% &      11.22\\\\% &      12.91\\\\% &      14.46\\\\% &        nan\\\\% &        nan\\\\% \\\\\\\\\\n     3 &      100 &       tanh &   34516 & 2.40\\\\% & 7.41\\\\% &       4.10\\\\% &       1.51\\\\% &       0.78\\\\% &        nan\\\\% &        nan\\\\% &        nan\\\\% \\\\\\\\\\n     2 &       20 &       tanh &   35098 & 2.72\\\\% & 4.81\\\\% &       7.50\\\\% &       9.94\\\\% &        nan\\\\% &        nan\\\\% &        nan\\\\% &        nan\\\\% \\\\\\\\\\n     4 &      100 &       tanh &   35817 & 2.93\\\\% & 7.83\\\\% &       1.31\\\\% &       1.20\\\\% &        nan\\\\% &        nan\\\\% &        nan\\\\% &        nan\\\\% \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_avg_errors.columns = [\"Layers\",\"Channels\",\"Activation\",\"Model file\",\"Test\",\"Sim\",\"Min sim\",\"Sim<37000.0\",\"Sim<36000.0\",\"Sim<35000.0\",\"Sim<34000.0\",\"Sim<33000.0\",\"Sim<32000.0\"]\n",
    "df_with_avg_errors_1 = df_with_avg_errors[[\"Layers\",\"Channels\",\"Activation\",\"Model file\",\"Min sim\",\"Test\",\"Sim\",\"Sim<37000.0\",\"Sim<36000.0\",\"Sim<35000.0\",\"Sim<34000.0\",\"Sim<33000.0\",\"Sim<32000.0\"]]\n",
    "\n",
    "perc_cols = [\"Test\",\"Sim\",\"Sim<37000.0\",\"Sim<36000.0\",\"Sim<35000.0\",\"Sim<34000.0\",\"Sim<33000.0\",\"Sim<32000.0\"]\n",
    "perc_format = {c: lambda x: \"{:.2%}\".format(x) for c in perc_cols}\n",
    "\n",
    "int_cols = [\"Layers\",\"Channels\",\"Min sim\"]\n",
    "int_format = {c: lambda x: str(int(x)) for c in int_cols}\n",
    "\n",
    "for k in int_format.keys():\n",
    "    perc_format[k] = int_format[k]\n",
    "\n",
    "df_with_avg_errors_1.style.format(perc_format)\n",
    "df_with_avg_errors_1[[\"Layers\",\"Channels\",\"Activation\",\"Min sim\",\"Test\",\"Sim\",\"Sim<37000.0\",\"Sim<36000.0\",\"Sim<35000.0\",\"Sim<34000.0\",\"Sim<33000.0\",\"Sim<32000.0\"]].sort_values(\"Min sim\")[0:15].to_latex(formatters=perc_format,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_avg_errors_1.to_csv(r'fit_eval_results_feedforward.csv', sep=',', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print stdevs table to LaTeX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{rrlrrrrrr}\\n\\\\toprule\\n\\\\#Lyr & \\\\#Ch &   Act & <37000.0 & <36000.0 & <35000.0 & <34000.0 & <33000.0 & <32000.0 \\\\\\\\\\n\\\\midrule\\n   2 & 100 &  tanh &    1.78\\\\% &    1.37\\\\% &    1.13\\\\% &    1.06\\\\% &    0.00\\\\% &     nan\\\\% \\\\\\\\\\n   2 &  40 &  tanh &    1.52\\\\% &    1.58\\\\% &    1.58\\\\% &    0.83\\\\% &    0.00\\\\% &     nan\\\\% \\\\\\\\\\n   4 &  40 &  tanh &    2.29\\\\% &    1.89\\\\% &    1.33\\\\% &    1.00\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   3 &  40 &  tanh &    1.66\\\\% &    1.71\\\\% &    1.87\\\\% &    1.50\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   3 &  20 &  tanh &    1.82\\\\% &    1.40\\\\% &    1.16\\\\% &    0.00\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   4 &  20 &  tanh &    2.44\\\\% &    1.97\\\\% &    0.99\\\\% &    0.00\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   3 & 100 &  tanh &    2.46\\\\% &    1.21\\\\% &    0.31\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   2 &  20 &  tanh &    2.13\\\\% &    0.80\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   4 & 100 &  tanh &    1.12\\\\% &    0.95\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_error_stdev.columns = [\"#Lyr\",\"#Ch\",\"Act\",\"Model file\",\"Err. test\",\"Err. sim\",\"Min sim\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]\n",
    "df_with_error_stdev_1 = df_with_error_stdev[[\"#Lyr\",\"#Ch\",\"Act\",\"Min sim\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]]\n",
    "\n",
    "perc_cols = [\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]\n",
    "perc_format = {c: lambda x: \"{:.2%}\".format(x) for c in perc_cols}\n",
    "\n",
    "int_cols = [\"#Lyr\",\"#Ch\",\"Min sim\"]\n",
    "int_format = {c: lambda x: str(int(x)) for c in int_cols}\n",
    "\n",
    "for k in int_format.keys():\n",
    "    perc_format[k] = int_format[k]\n",
    "\n",
    "df_with_error_stdev_1.style.format(perc_format)\n",
    "df_with_error_stdev_1[[\"#Lyr\",\"#Ch\",\"Act\",\"Min sim\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]].sort_values(\"Min sim\").loc[:,[\"#Lyr\",\"#Ch\",\"Act\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]][0:15].to_latex(formatters=perc_format,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_error_stdev.to_csv(r'fit_eval_results_stdevs_feedforward.csv', sep=',', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print df plus max errors to LaTeX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{rrlrrr}\\n\\\\toprule\\n \\\\#Lyr &  \\\\#Ch &   Act & Err. test & Err. sim & Max err. \\\\\\\\\\n\\\\midrule\\n    2 &  100 &  tanh &     1.85\\\\% &    3.37\\\\% &   10.20\\\\% \\\\\\\\\\n    2 &   40 &  tanh &     1.99\\\\% &    2.23\\\\% &   10.64\\\\% \\\\\\\\\\n    4 &   40 &  tanh &     1.89\\\\% &    3.73\\\\% &   10.55\\\\% \\\\\\\\\\n    3 &   40 &  tanh &     1.92\\\\% &    2.97\\\\% &    9.66\\\\% \\\\\\\\\\n    3 &   20 &  tanh &     2.67\\\\% &    2.37\\\\% &   12.79\\\\% \\\\\\\\\\n    4 &   20 &  tanh &     2.71\\\\% &    3.16\\\\% &   13.03\\\\% \\\\\\\\\\n    3 &  100 &  tanh &     2.40\\\\% &    7.41\\\\% &   13.18\\\\% \\\\\\\\\\n    2 &   20 &  tanh &     2.72\\\\% &    4.81\\\\% &   13.81\\\\% \\\\\\\\\\n    4 &  100 &  tanh &     2.93\\\\% &    7.83\\\\% &   15.23\\\\% \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plus_max_errors.columns = [\"#Lyr\",\"#Ch\",\"Act\",\"Model file\",\"Err. test\",\"Err. sim\",\"Min sim\", \"Max err.\"]\n",
    "\n",
    "perc_cols = [\"Err. test\",\"Err. sim\",\"Max err.\"]\n",
    "perc_format = {c: lambda x: \"{:.2%}\".format(x) for c in perc_cols}\n",
    "\n",
    "#for k in int_format.keys():\n",
    "#    perc_format[k] = int_format[k]\n",
    "\n",
    "df_plus_max_errors.style.format(perc_format)\n",
    "df_plus_max_errors[[\"#Lyr\",\"#Ch\",\"Act\",\"Err. test\",\"Err. sim\",\"Min sim\",\"Max err.\"]].sort_values(\"Min sim\").loc[:,[\"#Lyr\",\"#Ch\",\"Act\",\"Err. test\",\"Err. sim\",\"Max err.\"]][0:15].to_latex(formatters=perc_format,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plus_max_errors.to_csv(r'fit_eval_results_max_error_feedforward.csv', sep=',', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert test to sim differences and column averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{rrlrrrrrrrrrr}\\n\\\\toprule\\n\\\\#Lyr & \\\\#Ch &   Act & Min sim & Err. test & Err. sim & Err. sim-test & <37000.0 & <36000.0 & <35000.0 & <34000.0 & <33000.0 & <32000.0 \\\\\\\\\\n\\\\midrule\\n   2 & 100 &  tanh &   32921 &     1.85\\\\% &    3.37\\\\% &         1.52\\\\% &    2.63\\\\% &    1.89\\\\% &    1.81\\\\% &    2.52\\\\% &    4.33\\\\% &     nan\\\\% \\\\\\\\\\n   2 &  40 &  tanh &   32978 &     1.99\\\\% &    2.23\\\\% &         0.24\\\\% &    1.85\\\\% &    1.79\\\\% &    2.59\\\\% &    3.88\\\\% &    6.76\\\\% &     nan\\\\% \\\\\\\\\\n   4 &  40 &  tanh &   33117 &     1.89\\\\% &    3.73\\\\% &         1.84\\\\% &    4.58\\\\% &    5.66\\\\% &    6.97\\\\% &    8.59\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   3 &  40 &  tanh &   33536 &     1.92\\\\% &    2.97\\\\% &         1.05\\\\% &    2.47\\\\% &    2.19\\\\% &    3.29\\\\% &    5.26\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   3 &  20 &  tanh &   33922 &     2.67\\\\% &    2.37\\\\% &        -0.30\\\\% &    2.16\\\\% &    4.09\\\\% &    6.79\\\\% &   10.34\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   4 &  20 &  tanh &   33941 &     2.71\\\\% &    3.16\\\\% &         0.44\\\\% &    8.21\\\\% &   11.22\\\\% &   12.91\\\\% &   14.46\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   3 & 100 &  tanh &   34516 &     2.40\\\\% &    7.41\\\\% &         5.01\\\\% &    4.10\\\\% &    1.51\\\\% &    0.78\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   2 &  20 &  tanh &   35098 &     2.72\\\\% &    4.81\\\\% &         2.09\\\\% &    7.50\\\\% &    9.94\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   4 & 100 &  tanh &   35817 &     2.93\\\\% &    7.83\\\\% &         4.90\\\\% &    1.31\\\\% &    1.20\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% &     nan\\\\% \\\\\\\\\\n   3 &  53 &   NaN &   33982 &     2.34\\\\% &    4.21\\\\% &         1.87\\\\% &    3.87\\\\% &    4.39\\\\% &    5.02\\\\% &    7.51\\\\% &    5.55\\\\% &     nan\\\\% \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_avg_errors_1 = pd.read_csv(r'fit_eval_results_feedforward.csv', sep=',')\n",
    "df_with_avg_errors_1\n",
    "df_with_avg_errors_1[\"Err. sim-test\"]=df_with_avg_errors_1[\"Sim\"]-df_with_avg_errors_1[\"Test\"]\n",
    "df_with_avg_errors_1\n",
    "\n",
    "df_with_avg_errors_1.columns = [\"#Lyr\",\"#Ch\",\"Act\",\"Model file\",\"Min sim\",\"Err. test\",\"Err. sim\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\",\"Err. sim-test\"]\n",
    "df_with_avg_errors_1 = df_with_avg_errors_1[[\"#Lyr\",\"#Ch\",\"Act\",\"Model file\",\"Min sim\",\"Err. test\",\"Err. sim\",\"Err. sim-test\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]]\n",
    "\n",
    "perc_cols = [\"Err. test\",\"Err. sim\",\"Err. sim-test\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]\n",
    "perc_format = {c: lambda x: \"{:.2%}\".format(x) for c in perc_cols}\n",
    "\n",
    "int_cols = [\"#Lyr\",\"#Ch\",\"Min sim\"]\n",
    "int_format = {c: lambda x: str(int(x)) for c in int_cols}\n",
    "\n",
    "for k in int_format.keys():\n",
    "    perc_format[k] = int_format[k]\n",
    "\n",
    "df_with_avg_errors_1.style.format(perc_format)\n",
    "data_frame = df_with_avg_errors_1[[\"#Lyr\",\"#Ch\",\"Act\",\"Min sim\",\"Err. test\",\"Err. sim\",\"Err. sim-test\",\"<37000.0\",\"<36000.0\",\"<35000.0\",\"<34000.0\",\"<33000.0\",\"<32000.0\"]].sort_values(\"Min sim\")[0:15]\n",
    "column_averages = data_frame.mean(axis=0)\n",
    "data_frame = data_frame.append(column_averages,ignore_index=True)\n",
    "data_frame.to_latex(formatters=perc_format,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind]",
   "language": "python",
   "name": "conda-env-aind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
